{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms  # 是一个常用的图片变换类\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import prettytable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Side_length=8\n",
    "my_batch_size = Side_length*Side_length\n",
    "MyTransform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),  # 把数据转换成张量\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # 0.1307是均值，0.3081是标准差，具体如何计算得到，之后再看\n",
    "    ]\n",
    ")\n",
    "train_dataset = datasets.MNIST(root='./dataset/mnist',train=True,download=False, transform=MyTransform)  # #第一次使用需要设置为true，下载数据集,使用的图片变换类transforms\n",
    "train_process_data = DataLoader(train_dataset,  shuffle=True, batch_size=my_batch_size)   #shuffle：是否将数据打乱   #batch_size设置为my_batch_size\n",
    "test_dataset = datasets.MNIST(root='./dataset/mnist',train=False,download=False,transform=MyTransform)\n",
    "test_process_data = DataLoader(test_dataset,shuffle=True,batch_size=my_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCNN_NET(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyCNN_NET, self).__init__()  #初始化函数为空\n",
    "\n",
    "        self.L1_conv_pool = torch.nn.Sequential(  # torch.nn.Sequential理解为向网络中增加结构\n",
    "            torch.nn.Conv2d(1, 26, kernel_size=3), #卷积层，输入通道数目，输出通道数目，卷积核大小为3*3\n",
    "            torch.nn.BatchNorm2d(26),   #输入batch的每一个特征通道进行normalize，对26个通道进行normalize标准化\n",
    "            torch.nn.ReLU(inplace=True),  #激活函数relu  inplace为True，将会改变输入的数据 \n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)  \n",
    "            #kernel_size ：表示做最大池化的窗口大小，可以是单个值，也可以是tuple元组\n",
    "            #stride ：步长，可以是单个值，也可以是tuple元组\n",
    "        )\n",
    "\n",
    "        self.L2_conv_pool = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(26, 52, kernel_size=3),  #输出通道数52,长度和宽度各减2\n",
    "            torch.nn.BatchNorm2d(52),\n",
    "            torch.nn.ReLU(inplace=True) ,\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2) \n",
    "        )\n",
    "\n",
    "        self.L3_fc = torch.nn.Sequential(  #全连接层\n",
    "            torch.nn.Linear(52 * 5 * 5, 1024),  #由52*5*5变到1024*1，变到128*1，到10*1\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(1024, 128),\n",
    "            torch.nn.ReLU(inplace=True),\n",
    "            torch.nn.Linear(128, 10)\n",
    "        )\n",
    "    \n",
    "    def ForwardPropagation(self, x):\n",
    "        x = self.L1_conv_pool(x)\n",
    "        x = self.L2_conv_pool(x)\n",
    "        x = x.view(x.size(0), -1)  # 在进入全连接层之前需要把数据拉直Flatten  \n",
    "        # view()的作用相当于numpy中的reshape，重新定义矩阵的形状。\n",
    "        #变行数为x.size(0)，列数随着其自动调整\n",
    "        x = self.L3_fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train===0\n",
      "epoch 1 percent: 10% loss:0.7518920\n",
      "epoch 1 percent: 20% loss:0.1710116\n",
      "epoch 1 percent: 30% loss:0.1355203\n",
      "epoch 1 percent: 40% loss:0.0922126\n",
      "epoch 1 percent: 50% loss:0.0694858\n",
      "epoch 1 percent: 60% loss:0.0719873\n",
      "epoch 1 percent: 70% loss:0.0593762\n",
      "epoch 1 percent: 80% loss:0.0621834\n",
      "epoch 1 percent: 90% loss:0.0564769\n",
      "test===0\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "Accuracy（test） == 98.440000 %\n",
      "train===0\n",
      "epoch 2 percent: 10% loss:0.0401990\n",
      "epoch 2 percent: 20% loss:0.0484504\n",
      "epoch 2 percent: 30% loss:0.0413558\n",
      "epoch 2 percent: 40% loss:0.0436657\n",
      "epoch 2 percent: 50% loss:0.0354218\n",
      "epoch 2 percent: 60% loss:0.0467803\n",
      "epoch 2 percent: 70% loss:0.0409082\n",
      "epoch 2 percent: 80% loss:0.0382148\n",
      "epoch 2 percent: 90% loss:0.0398346\n",
      "test===0\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "Accuracy（test） == 98.780000 %\n",
      "train===0\n",
      "epoch 3 percent: 10% loss:0.0260264\n",
      "epoch 3 percent: 20% loss:0.0260472\n",
      "epoch 3 percent: 30% loss:0.0280562\n",
      "epoch 3 percent: 40% loss:0.0237475\n",
      "epoch 3 percent: 50% loss:0.0289673\n",
      "epoch 3 percent: 60% loss:0.0302204\n",
      "epoch 3 percent: 70% loss:0.0316720\n",
      "epoch 3 percent: 80% loss:0.0279145\n",
      "epoch 3 percent: 90% loss:0.0304621\n",
      "test===0\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "Accuracy（test） == 97.820000 %\n",
      "train===0\n",
      "epoch 4 percent: 10% loss:0.0194980\n",
      "epoch 4 percent: 20% loss:0.0179973\n",
      "epoch 4 percent: 30% loss:0.0246623\n",
      "epoch 4 percent: 40% loss:0.0205095\n",
      "epoch 4 percent: 50% loss:0.0211917\n",
      "epoch 4 percent: 60% loss:0.0218153\n",
      "epoch 4 percent: 70% loss:0.0193359\n",
      "epoch 4 percent: 80% loss:0.0204389\n",
      "epoch 4 percent: 90% loss:0.0214473\n",
      "test===0\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "Accuracy（test） == 99.180000 %\n",
      "train===0\n",
      "epoch 5 percent: 10% loss:0.0090158\n",
      "epoch 5 percent: 20% loss:0.0126805\n",
      "epoch 5 percent: 30% loss:0.0168218\n",
      "epoch 5 percent: 40% loss:0.0147142\n",
      "epoch 5 percent: 50% loss:0.0141070\n",
      "epoch 5 percent: 60% loss:0.0170129\n",
      "epoch 5 percent: 70% loss:0.0154793\n",
      "epoch 5 percent: 80% loss:0.0180207\n",
      "epoch 5 percent: 90% loss:0.0189271\n",
      "test===0\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "Accuracy（test） == 99.210000 %\n",
      "train===0\n",
      "epoch 6 percent: 10% loss:0.0098362\n",
      "epoch 6 percent: 20% loss:0.0077823\n",
      "epoch 6 percent: 30% loss:0.0126516\n",
      "epoch 6 percent: 40% loss:0.0118742\n",
      "epoch 6 percent: 50% loss:0.0091184\n",
      "epoch 6 percent: 60% loss:0.0111588\n",
      "epoch 6 percent: 70% loss:0.0115685\n",
      "epoch 6 percent: 80% loss:0.0113226\n",
      "epoch 6 percent: 90% loss:0.0115580\n",
      "test===0\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "Accuracy（test） == 98.210000 %\n",
      "train===0\n",
      "epoch 7 percent: 10% loss:0.0074358\n",
      "epoch 7 percent: 20% loss:0.0067944\n",
      "epoch 7 percent: 30% loss:0.0103755\n",
      "epoch 7 percent: 40% loss:0.0076528\n",
      "epoch 7 percent: 50% loss:0.0071380\n",
      "epoch 7 percent: 60% loss:0.0078606\n",
      "epoch 7 percent: 70% loss:0.0096258\n",
      "epoch 7 percent: 80% loss:0.0075334\n",
      "epoch 7 percent: 90% loss:0.0083416\n",
      "test===0\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "Accuracy（test） == 99.250000 %\n",
      "train===0\n",
      "epoch 8 percent: 10% loss:0.0043537\n",
      "epoch 8 percent: 20% loss:0.0021473\n",
      "epoch 8 percent: 30% loss:0.0060972\n",
      "epoch 8 percent: 40% loss:0.0055559\n",
      "epoch 8 percent: 50% loss:0.0070169\n",
      "epoch 8 percent: 60% loss:0.0035949\n",
      "epoch 8 percent: 70% loss:0.0051756\n",
      "epoch 8 percent: 80% loss:0.0058049\n",
      "epoch 8 percent: 90% loss:0.0108085\n",
      "test===0\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "Accuracy（test） == 99.200000 %\n",
      "train===0\n",
      "epoch 9 percent: 10% loss:0.0031955\n",
      "epoch 9 percent: 20% loss:0.0050368\n",
      "epoch 9 percent: 30% loss:0.0038175\n",
      "epoch 9 percent: 40% loss:0.0053916\n",
      "epoch 9 percent: 50% loss:0.0029362\n",
      "epoch 9 percent: 60% loss:0.0039352\n",
      "epoch 9 percent: 70% loss:0.0040044\n",
      "epoch 9 percent: 80% loss:0.0040091\n",
      "epoch 9 percent: 90% loss:0.0049902\n",
      "test===0\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "Accuracy（test） == 99.360000 %\n",
      "train===0\n",
      "epoch 10 percent: 10% loss:0.0012902\n",
      "epoch 10 percent: 20% loss:0.0023050\n",
      "epoch 10 percent: 30% loss:0.0022971\n",
      "epoch 10 percent: 40% loss:0.0030518\n",
      "epoch 10 percent: 50% loss:0.0019736\n",
      "epoch 10 percent: 60% loss:0.0056129\n",
      "epoch 10 percent: 70% loss:0.0043209\n",
      "epoch 10 percent: 80% loss:0.0028852\n",
      "epoch 10 percent: 90% loss:0.0036908\n",
      "test===0\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "Accuracy（test） == 99.400000 %\n",
      "train===0\n",
      "epoch 11 percent: 10% loss:0.0027464\n",
      "epoch 11 percent: 20% loss:0.0031540\n",
      "epoch 11 percent: 30% loss:0.0034625\n",
      "epoch 11 percent: 40% loss:0.0030056\n",
      "epoch 11 percent: 50% loss:0.0017855\n",
      "epoch 11 percent: 60% loss:0.0011042\n",
      "epoch 11 percent: 70% loss:0.0018400\n",
      "epoch 11 percent: 80% loss:0.0021132\n",
      "epoch 11 percent: 90% loss:0.0012462\n",
      "test===0\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "Accuracy（test） == 99.360000 %\n",
      "train===0\n",
      "epoch 12 percent: 10% loss:0.0008237\n",
      "epoch 12 percent: 20% loss:0.0011985\n",
      "epoch 12 percent: 30% loss:0.0008353\n",
      "epoch 12 percent: 40% loss:0.0021105\n",
      "epoch 12 percent: 50% loss:0.0010943\n",
      "epoch 12 percent: 60% loss:0.0010633\n",
      "epoch 12 percent: 70% loss:0.0006708\n",
      "epoch 12 percent: 80% loss:0.0009883\n",
      "epoch 12 percent: 90% loss:0.0015277\n",
      "test===0\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "Accuracy（test） == 99.250000 %\n",
      "train===0\n",
      "epoch 13 percent: 10% loss:0.0005203\n",
      "epoch 13 percent: 20% loss:0.0013440\n",
      "epoch 13 percent: 30% loss:0.0004891\n",
      "epoch 13 percent: 40% loss:0.0004233\n",
      "epoch 13 percent: 50% loss:0.0007863\n",
      "epoch 13 percent: 60% loss:0.0005270\n",
      "epoch 13 percent: 70% loss:0.0015821\n",
      "epoch 13 percent: 80% loss:0.0003215\n",
      "epoch 13 percent: 90% loss:0.0004741\n",
      "test===0\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "Accuracy（test） == 99.340000 %\n",
      "train===0\n",
      "epoch 14 percent: 10% loss:0.0008178\n",
      "epoch 14 percent: 20% loss:0.0004986\n",
      "epoch 14 percent: 30% loss:0.0005045\n",
      "epoch 14 percent: 40% loss:0.0004789\n",
      "epoch 14 percent: 50% loss:0.0003623\n",
      "epoch 14 percent: 60% loss:0.0004092\n",
      "epoch 14 percent: 70% loss:0.0003938\n",
      "epoch 14 percent: 80% loss:0.0004717\n",
      "epoch 14 percent: 90% loss:0.0003949\n",
      "test===0\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "Accuracy（test） == 99.430000 %\n",
      "train===0\n",
      "epoch 15 percent: 10% loss:0.0002517\n",
      "epoch 15 percent: 20% loss:0.0004847\n",
      "epoch 15 percent: 30% loss:0.0004867\n",
      "epoch 15 percent: 40% loss:0.0004431\n",
      "epoch 15 percent: 50% loss:0.0004202\n",
      "epoch 15 percent: 60% loss:0.0004441\n",
      "epoch 15 percent: 70% loss:0.0003579\n",
      "epoch 15 percent: 80% loss:0.0004032\n",
      "epoch 15 percent: 90% loss:0.0003401\n",
      "test===0\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "Accuracy（test） == 99.390000 %\n",
      "train===0\n",
      "epoch 16 percent: 10% loss:0.0004676\n",
      "epoch 16 percent: 20% loss:0.0002019\n",
      "epoch 16 percent: 30% loss:0.0001718\n",
      "epoch 16 percent: 40% loss:0.0002685\n",
      "epoch 16 percent: 50% loss:0.0003759\n",
      "epoch 16 percent: 60% loss:0.0002322\n",
      "epoch 16 percent: 70% loss:0.0002891\n",
      "epoch 16 percent: 80% loss:0.0001777\n",
      "epoch 16 percent: 90% loss:0.0001838\n",
      "test===0\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "Accuracy（test） == 99.370000 %\n",
      "train===0\n",
      "epoch 17 percent: 10% loss:0.0001496\n",
      "epoch 17 percent: 20% loss:0.0002201\n",
      "epoch 17 percent: 30% loss:0.0002035\n",
      "epoch 17 percent: 40% loss:0.0002169\n",
      "epoch 17 percent: 50% loss:0.0004783\n",
      "epoch 17 percent: 60% loss:0.0002204\n",
      "epoch 17 percent: 70% loss:0.0002255\n",
      "epoch 17 percent: 80% loss:0.0001455\n",
      "epoch 17 percent: 90% loss:0.0002667\n",
      "test===0\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "Accuracy（test） == 99.380000 %\n",
      "train===0\n",
      "epoch 18 percent: 10% loss:0.0000919\n",
      "epoch 18 percent: 20% loss:0.0001943\n",
      "epoch 18 percent: 30% loss:0.0001579\n",
      "epoch 18 percent: 40% loss:0.0001383\n",
      "epoch 18 percent: 50% loss:0.0001695\n",
      "epoch 18 percent: 60% loss:0.0001851\n",
      "epoch 18 percent: 70% loss:0.0002038\n",
      "epoch 18 percent: 80% loss:0.0002006\n",
      "epoch 18 percent: 90% loss:0.0001688\n",
      "test===0\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "Accuracy（test） == 99.440000 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train(train_idx):\n",
    "    print(\"train===0\")\n",
    "    cross_loss = 0.0\n",
    "    for train_id, data in enumerate(train_process_data, 0):  #每次一个batch（64）\n",
    "        Feature, Tag = data\n",
    "        Feature, Tag = Feature.to(device), Tag.to(device)  # 将数据放在GPU上跑所需要的代码\n",
    "        MyOptimizer.zero_grad()  #optimizer.zero_grad()意思是把梯度置零，也就是把loss关于weight的导数变成0.\n",
    "        outputs = model_CNN.ForwardPropagation(Feature) \n",
    "        # print(\"---\",target.shape)\n",
    "        # print(\"====\",outputs.shape)\n",
    "        # print(target)\n",
    "        LossFunction=torch.nn.CrossEntropyLoss()\n",
    "        loss = LossFunction(outputs, Tag)  #对target进行onehot编码\n",
    "        loss.backward()         \n",
    "        #Backward函数实际上是通过传递参数(默认情况下是1x1单位张量)来计算梯度的，\n",
    "        # 它通过Backward图一直到每个叶节点，每个叶节点都可以从调用的根张量追溯到叶节点。然后将计算出的梯度存储在每个叶节点的.grad中\n",
    "        MyOptimizer.step()       # 所有的optimizer都实现了step()方法，这个方法会更新所有的参数。 \n",
    "\n",
    "        #优化器就是需要根据网络反向传播的梯度信息来更新网络的参数，以起到降低loss函数计算值的作用，这也是机器学习里面最一般的方法论。\n",
    "        #step这个函数使用的是参数空间(param_groups)中的grad,也就是当前参数空间对应的梯度\n",
    "        #解释了为什么optimzier使用之前需要zero清零一下，因为如果不清零，那么使用的这个grad就得同上一个mini-batch有关，这不是我们需要的结果\n",
    "        #我们知道optimizer更新参数空间需要基于反向梯度，因此，当调用optimizer.step()的时候应当是loss.backward()的时候\n",
    "        #那么为什么optimizer.step()需要放在每一个batch训练中，而不是epoch训练中，这是因为现在的mini-batch训练模式是假定每一个训练集就\n",
    "        # 只有mini-batch这样大，因此实际上可以将每一次mini-batch看做是一次训练，一次训练更新一次参数空间，因而optimizer.step()放在这里。\n",
    "        cross_loss += loss.item()\n",
    "        #代码中所有的loss都直接用loss表示的，结果就是每次迭代，空间占用就会增加，直到cpu或者gup爆炸。\n",
    "        # 解决办法：把除了loss.backward()之外的loss调用都改成loss.item()，就可以解决。\n",
    "        #可以看出是显示精度的区别，item()返回的是一个浮点型数据（更精确），所以我们在求loss或者accuracy时，一般使用item()，而不是直接取它对应的元素x[1,1]。\n",
    "        \n",
    "        if train_id % 100 == 0 and train_id!=0:  # 不让他每一次小的迭代就输出，而是300次小迭代再输出一次\n",
    "            print(\"epoch %d\" % train_idx,end=\" \")\n",
    "            print('percent: {:.00%}'.format(train_id/1000),end=\" \")\n",
    "            print('loss:%.7f' % (cross_loss/100),end=\"\")\n",
    "            print(\"\")\n",
    "            # print('epoch %d,{:.2f}% loss:%.7f' % (train_idx, (train_id + 1)/10, cross_loss / 100))\n",
    "            cross_loss = 0.0\n",
    "    torch.save(model_CNN, './modelpth/model_new_{}.pth'.format(train_idx))\n",
    "\n",
    "\n",
    "def test():\n",
    "    print(\"test===0\")\n",
    "    correct_sum = 0\n",
    "    total_sum = 0\n",
    "    with torch.no_grad():  # 下面的代码就不会再计算梯度，torch.no_grad()是一个上下文管理器，用来禁止梯度的计算，通常用来网络推断中，它可以减少计算内存的使用量。\n",
    "        for data in test_process_data:\n",
    "            Feature, Tag = data\n",
    "            Feature, Tag = Feature.to(device), Tag.to(device)  # 将数据放在GPU上跑所需要的代码\n",
    "            outputs = model_CNN.ForwardPropagation(Feature)\n",
    "            Row_Max , Max_Index = torch.max(outputs.data, dim=1)  # Row_Max为每一行的最大值，Max_Index表示每一行最大值的下标\n",
    "            #input是softmax函数输出的一个tensor ， dim是max函数索引的维度0/1，0是每列的最大值，1是每行的最大值\n",
    "            total_sum += Tag.size(0)  #计算样本总数\n",
    "            correct_sum += (Max_Index == Tag).sum().item()  \n",
    "    print(type(correct_sum))\n",
    "    print(type(total_sum))\n",
    "    print('Accuracy（test） == %.6f %%' % (100 * correct_sum / total_sum))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model_CNN = MyCNN_NET()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # 将数据放在GPU上跑所需要的代码\n",
    "    model_CNN.to(device)  # 将数据放在GPU上跑所需要的代码\n",
    "    MyOptimizer = torch.optim.SGD(model_CNN.parameters(), lr=0.1)  \n",
    "    for train_idx in range(1,19):\n",
    "        train(train_idx)\n",
    "        test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.1 ('pytranscd37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "335ddbc004233a7b01ddb2f89126eb9ef318bdab1b2646dc45405f096632921a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
